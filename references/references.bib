% Example bib item
@inproceedings{key1,
title     = "Title of the conference paper",
author    = "John Author",
booktitle = "Name of the Conference Proceedings",
address   = "Location",
month     = "Mon.",
year      = "Year"
}

% #############################
% Description Logic references
% #############################
@book{DLhandbook,
author = {van Harmelen, Frank and van Harmelen, Frank and Lifschitz, Vladimir and Porter, Bruce},
title = {Handbook of Knowledge Representation},
year = {2007},
isbn = {0444522115},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
abstract = {Knowledge Representation, which lies at the core of Artificial Intelligence, is concerned with encoding knowledge on computers to enable systems to reason automatically. The Handbook of Knowledge Representation is an up-to-date review of twenty-five key topics in knowledge representation, written by the leaders of each field.This book is an essential resource for students, researchers and practitioners in all areas of Artificial Intelligence. * Make your computer smarter* Handle qualitative and uncertain information* Improve computational tractability to solve your problems easily}
}

% Introduces the description logic ALC and shows its complexity
@article{ALCcomplexity,
title = {Attributive concept descriptions with complements},
journal = {Artificial Intelligence},
volume = {48},
number = {1},
pages = {1-26},
year = {1991},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(91)90078-X},
url = {https://www.sciencedirect.com/science/article/pii/000437029190078X},
author = {Manfred Schmidt-Schauß and Gert Smolka},
abstract = {We investigate the consequences of adding unions and complements to attributive concept descriptions employed in terminological knowledge representation languages. It is shown that deciding coherence and subsumption of such descriptions are PSPACE-complete problems that can be decided with linear space.}
}

% Introduces the description logic EL and shows its complexity
@inproceedings{ELcomplexity,
author = {Baader, Franz},
title = {Terminological Cycles in a Description Logic with Existential Restrictions},
year = {2003},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Cyclic definitions in description logics have until now been investigated only for description logics allowing for value restrictions. Even for the most basic language FL0, which allows for conjunction and value restrictions only, deciding subsumption in the presence of terminological cycles is a PSPACE-complete problem. This paper investigates subsumption in the presence of terminological cycles for the language EL, Which allows for conjunction, existential restrictions, and the topconcept. In contrast to the results for FL0., subsumption in EL remains polynomial, independent of whether we use least fixpoint semantics, greatest fixpoint semantics, or descriptive semantics.},
booktitle = {Proceedings of the 18th International Joint Conference on Artificial Intelligence},
pages = {325–330},
numpages = {6},
location = {Acapulco, Mexico},
series = {IJCAI'03}
}

% Introduces the description logic EL++ and shows its complexity
@inproceedings{EL++complexity,
author = {Baader, Franz and Brandt, Sebastian and Lutz, Carsten},
title = {Pushing the EL Envelope},
year = {2005},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Recently, it has been shown that the small description logic (DL) EL, which allows for conjunction and existential restrictions, has better algorithmic properties than its counterpart FL0, which allows for conjunction and value restrictions. Whereas the subsumption problem in FL0 becomes already intractable in the presence of acyclic TBoxes, it remains tractable in EL even with general concept inclusion axioms (GCIs). On the one hand, we extend the positive result for EL by identifying a set of expressive means that can be added to EL without sacrificing tractability. On the other hand, we show that basically all other additions of typical DL constructors to EL with GCIs make subsumption intractable, and in most cases even EXPTIME-complete. In addition, we show that subsumption in FL0 with GCIs is EXPTIME-complete.},
booktitle = {Proceedings of the 19th International Joint Conference on Artificial Intelligence},
pages = {364–369},
numpages = {6},
location = {Edinburgh, Scotland},
series = {IJCAI'05}
}

% #############################
% Artificial Intelligence references
% #############################
@article{SurveryKG,
	doi = {10.1109/tnnls.2021.3070843},
  
	url = {https://doi.org/10.1109%2Ftnnls.2021.3070843},
  
	year = 2022,
	month = {feb},
  
	publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  
	volume = {33},
  
	number = {2},
  
	pages = {494--514},
  
	author = {Shaoxiong Ji and Shirui Pan and Erik Cambria and Pekka Marttinen and Philip S. Yu},
  
	title = {A Survey on Knowledge Graphs: Representation, Acquisition, and Applications},
  
	journal = {{IEEE} Transactions on Neural Networks and Learning Systems}
}

% The first paper on word-count based embeddings
@article{LatentSemanticAnalysis,
author = {Deerwester, Scott and Dumais, Susan T. and Furnas, George W. and Landauer, Thomas K. and Harshman, Richard},
title = {Indexing by latent semantic analysis},
journal = {Journal of the American Society for Information Science},
volume = {41},
number = {6},
pages = {391-407},
doi = {https://doi.org/10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9},
url = {https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-4571%28199009%2941%3A6%3C391%3A%3AAID-ASI1%3E3.0.CO%3B2-9},
abstract = {Abstract A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (“semantic structure”) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. Initial tests find this completely automatic method for retrieval to be promising. © 1990 John Wiley \& Sons, Inc.},
year = {1990}
}

@Article{ThePeakingPhenomenon,
  author={Amin Zollanvari and Alex Pappachen James and Reza Sameni},
  title={{A Theoretical Analysis of the Peaking Phenomenon in Classification}},
  journal={Journal of Classification},
  year=2020,
  volume={37},
  number={2},
  pages={421-434},
  month={July},
  keywords={Peaking phenomenon; Linear discriminant analysis; Classification error rate; Multiple asymptotic ana},
  doi={10.1007/s00357-019-09327-},
  abstract={ In this work, we analytically study the peaking phenomenon in the context of linear discriminant analysis in the multivariate Gaussian model under the assumption of a common known covariance matrix. The focus is finite-sample setting where the sample size and observation dimension are comparable. Therefore, in order to study the phenomenon in such a setting, we use an asymptotic technique whereby the number of sample points is kept comparable in magnitude to the dimensionality of observations. The analysis provides a more thorough picture of the phenomenon. In particular, the analysis shows that as long as the Relative Cumulative Efficacy of an additional Feature set (RCEF) is greater (less) than the size of this set, the expected error of the classifier constructed using these additional features will be less (greater) than the expected error of the classifier constructed without them. Our result highlights underlying factors of the peaking phenomenon relative to the classifier used in this study and, at the same time, calls into question the classical wisdom around the peaking phenomenon.},
  url={https://ideas.repec.org/a/spr/jclass/v37y2020i2d10.1007_s00357-019-09327-3.html}
}


@inproceedings{WhenIsNearestNeighborMeaningful,
	abstract = {We explore the effect of dimensionality on the ``nearest neighbor'' problem. We show that under a broad set of conditions (much broader than independent and identically distributed dimensions), as dimensionality increases, the distance to the nearest data point approaches the distance to the farthest data point. To provide a practical perspective, we present empirical results on both real and synthetic data sets that demonstrate that this effect can occur for as few as 10--15 dimensions.},
	address = {Berlin, Heidelberg},
	author = {Beyer, Kevin and Goldstein, Jonathan and Ramakrishnan, Raghu and Shaft, Uri},
	booktitle = {Database Theory --- ICDT'99},
	editor = {Beeri, Catriel and Buneman, Peter},
	isbn = {978-3-540-49257-3},
	pages = {217--235},
	publisher = {Springer Berlin Heidelberg},
	title = {When Is ``Nearest Neighbor'' Meaningful?},
	year = {1999}}

@article{SurveryDistanceIndiscernabilityResults,
author = {Zimek, Arthur and Schubert, Erich and Kriegel, Hans-Peter},
title = {A survey on unsupervised outlier detection in high-dimensional numerical data},
journal = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
volume = {5},
number = {5},
pages = {363-387},
keywords = {curse of dimensionality, anomalies in high-dimensional data, outlier detection in high-dimensional data, approximate outlier detection, subspace outlier detection, correlation outlier detection},
doi = {https://doi.org/10.1002/sam.11161},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sam.11161},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sam.11161},
abstract = {Abstract High-dimensional data in Euclidean space pose special challenges to data mining algorithms. These challenges are often indiscriminately subsumed under the term ‘curse of dimensionality’, more concrete aspects being the so-called ‘distance concentration effect’, the presence of irrelevant attributes concealing relevant information, or simply efficiency issues. In about just the last few years, the task of unsupervised outlier detection has found new specialized solutions for tackling high-dimensional data in Euclidean space. These approaches fall under mainly two categories, namely considering or not considering subspaces (subsets of attributes) for the definition of outliers. The former are specifically addressing the presence of irrelevant attributes, the latter do consider the presence of irrelevant attributes implicitly at best but are more concerned with general issues of efficiency and effectiveness. Nevertheless, both types of specialized outlier detection algorithms tackle challenges specific to high-dimensional data. In this survey article, we discuss some important aspects of the ‘curse of dimensionality’ in detail and survey specialized algorithms for outlier detection from both categories. © 2012 Wiley Periodicals, Inc. Statistical Analysis and Data Mining, 2012},
year = {2012}
}

@article{FirstPaperOnAIEmbeddings,
author = {Bengio, Yoshua and Ducharme, R\'{e}jean and Vincent, Pascal and Janvin, Christian},
title = {A Neural Probabilistic Language Model},
year = {2003},
issue_date = {3/1/2003},
publisher = {JMLR.org},
volume = {3},
number = {null},
issn = {1532-4435},
abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
journal = {J. Mach. Learn. Res.},
month = {mar},
pages = {1137–1155},
numpages = {19}
}


@article{AgnosticPAC,
	author = {Vapnik, V. N. and Chervonenkis, A. Ya.},
	doi = {10.1137/1116025},
	eprint = {https://doi.org/10.1137/1116025},
	journal = {Theory of Probability \& Its Applications},
	number = {2},
	pages = {264-280},
	title = {On the Uniform Convergence of Relative Frequencies of Events to Their Probabilities},
	url = {https://doi.org/10.1137/1116025},
	volume = {16},
	year = {1971},
	bdsk-url-1 = {https://doi.org/10.1137/1116025}
}

