
% Appendix A File

\refstepcounter{chapter}%
\chapter*{\thechapter \quad Neuro-symbolic AI}
\label{appendixB}
In this appendix, we provide a brief introduction to neuro-symbolic AI in relation to knowledge bases.
We would start by providing a motivation for the usage of knowledge bases in AI and then we would
provide a brief introduction to the AI algorithms that use knowledge bases.

\section{Motivation - Knowledge Base Embeddings}

Description logics are primarly used in the field of artifical intelligence in the context of knowledge base embeddings.
Knowledge base embeddings are methods of representing a knowledge base in some euclidean space $\mathbb{R}^n$.
Given a knowledge base $\mathcal{K}$, a knowledge base embedding is a function
\[
    \mathcal{E} : \dlconcepts \rightarrow \mathbb{R}^n
\]
that maps the concepts in the knowledge base to some point in $\mathbb{R}^n$ 
in such a way that some structural properties of the knowledge base are preserved. The embeddings are then used
for different knowledge representation and Natural Language Processing tasks \cite{SurveryKG}.
Some of these tasks include
\begin{itemize}
    \item \textbf{Knowledge Base Completion:} Applying the embeddings to a knowledge base allows us to
    exploit the fact that eucalidean spaces have a notion of distance between points.
    This allows for a notion of similarity between entities in the knowledge base
    which can be used to predict new facts.
    \par In large knowledge bases, it is often the case that solving queries is computationally expensive.
    The embeddings can be used to speed up the process of solving queries by using the embeddings to
    predict the answer to the query.
    \par Furthermore, the embeddings can be used to predict new facts that cannot be inferred from the knowledge base
    but are likely to be true.
    For instance, given the fact that "Steve Jobs was the CEO of Apple", we can predict that "Steve Jobs was the founder of Apple"
    since the two facts are similar.
    \item \textbf{Entity linking and Relation Extraction:} Applying the embeddings to a knowledge base allows for
    the usage of the knowledge base as features in Natural Language Processing (NLP) tasks.
    Two of the most common NLP tasks for KB embeddings are entity linking and relation extraction.
    \par Entity linking is the task of identifying the entities mentioned in a text and linking them to their corresponding
    entities in the knowledge base.
    For instance, given the term "Apple", entity linking is the task of identifying whether the term refers to the company or the fruit.
    This is done by computing the similarity between the term and the embeddings of the entities in the knowledge base,
    which is a task that can be done efficiently in euclidean spaces.
    \par Relation extraction is the task of identifying the relations between entities in a text.
    For instance, given the text "Steve Jobs was the CEO of Apple", relation extraction is the task of identifying that
    the relation between the entities "Steve Jobs" and "Apple" is "CEO".
    This task can be done using KBs by linking the names in the text to their corresponding entities in the KB (e.g. "Steve Jobs" to the entity "Person"
    and "Apple" to the entity "Company"), a task often referred to as name-entity recognition, and then using the KB to identify the relation between the entities.
    As shown below, AI tasks require a loss function that measures the similarity between 
    the predicted output of the AI algorithm and the ground truth.
    The embeddings can be used to numerically represent and measure the similarity between the predicted output and the ground truth.    
\end{itemize}

Furthermore, the embeddings can be used to produce a universal schema for the knowledge base. 
This allows us to easily extend the knowledge base with new information by simply adding new entities and relations to the embedding. As we would describe later
the behaviour of AI algorithms, we would see that this prevents us from having to retrain the AI algorithm from scratch when new information is added to the knowledge base.

In this thesis, we focus on embedding functions that are generated by AI algorithms. To motivate
the usage of AI algorithms for generating embedding functions, we would first describe older methods of generating embedding functions.
These methods were primarly used for the task of word embeddings, which is the task of representing words in a way that captures the 
likelihood of a word to appear in a given context. The distance between the embeddings of two words then represents the likelihood
of those words to appear in the same context.
The original embedding methods were algebraic methods that were based on computing the output of a function that was applied to the word.

For instance in \cite{LatentSemanticAnalysis}, the embedding function was computed using latent semantic analysis (LSA) to generate a word embedding.
The purpose of word embeddings is to represents words in a way that captures the likelihood of a word to appear in a given context.
A common algebraic method for computing word embeddings are latent semantic analysis (LSA) \cite{LatentSemanticAnalysis}.
In LSA, the algorithm is provided with a corpus $\{d_i\}_{i\in\mathcal{D}}$.
A term is then represented by $\{c_i\}_{i\in\mathcal{D}}$ where
$c_i$ is the number of occurences of the term in document $d_i$.

Intuitively, we would expect that the larger our corpus is (i.e. the more dimensions our vector space has),
the more accurate our word embeddings would be.
However, empirically and theoratically we observe that as the amount of dimensions
increases, the amount of data required to train any AI model increases exponentially.
This is often known as the \it{peaking phenomenon} \cite{ThePeakingPhenomenon}.

To address this issue, embedding algorithms incorprate
a \it{dimensionality reduction} procedure that a. identifies the most impactful dimensions and 
b. removes the least important dimensions.
For instance, in LSA, a dimensionality reduction step is done using singular value decomposition (SVD).
The step would identify important classes of documents and project the original 
vector space into a lower dimensional space. From this example, we learn that we favour embedding algorithms that are able to generate embeddings in a lower dimensional space
and are able to identify the most important dimensions in the embedding space.

AI algorithms take a different approach to generating embedding functions.
Instead of computing the embedding function directly, AI algorithms generate the embedding function in a process known as training
by optimizing a loss function that measures the similarity between the predicted output of the AI algorithm and the ground truth
(here being the embeddings of the entities in the knowledge base).
At each step of the training process, the AI algorithm would
receive a set of inputs and produce a set of outputs. The loss function would then measure the similarity between the predicted output and the ground truth
(e.g. the structure of the knowledge base that we are trying to embed).
The AI algorithm would then adjust the parameters of the embedding function in such a way that the loss function is minimized.
This process is repeated until sufficiently desired.

Instead of having a dimensionality reduction step, the dimensions of the embedding space remain fixed and low dimensional.
The low dimensionality of the embedding space forces the AI algorithm to identify the most optimal dimensions in the embedding space
to preserve the structure of the knowledge base while still remaining in a low dimensional space.
Empirically, we observe that AI algorithms are able to generate embedding functions that are more accurate than algebraic methods \cite{SurveryKG}.

We would dedicate the rest of this appendix to describing the mathematical details of AI algorithms which lead to them being able to generate embedding functions.