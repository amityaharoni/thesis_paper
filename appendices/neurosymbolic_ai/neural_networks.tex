\section{Artificial Neural Networks}
We saw that for any
finite hypothesis class $\mathcal{H}$ it is possible to find a polynomial time 
algorithm that can get sufficiently close to the ERM solution by 
minimizing the empirical loss function.

Given a sample space $X$,
we can think of the hypothesis class $\mathcal{H}$ as
a function $f:\Theta\to\mathcal{H}$
where $\Theta$ is a set of hyperparameters 
that fix a single hypothesis in $\mathcal{H}$.
If $f$ is continuous, then we can find the ERM solution by
taking the derivative of the empirical loss function with respect to $\Theta$ and setting it to $0$.
\[
    \frac{\partial}{\partial\Theta}\mathcal{L}_{\mathbb{D}}(f(\Theta))=0    
\]
We can then use algebraic methods to solve for $\Theta$. 
This approach relies on having a nicely behaved function $f$ for which we can solve
the differential equation above.
However in practice it is difficult to identify the hypothesis class $\mathcal{H}$ 
with a nicely behaving function $f$.
We can however identify a function $f$ that can approximate many hypothesis classes
and use a version of gradient descent to find the ERM solution.
The function $f$ that we would use is called a neural network and the learning
algorithm is called stochastic gradient descent.

However, we wish to find a methodological way to generate a learning algorithm
that can get sufficiently close to the ERM solution for any hypothesis class.
This is the goal of the field of \it{machine learning}.
Neural networks are a family of machine learning algorithms that have been shown to be
effective in practice for a wide range of tasks. 
\subsection{Architecture}
\begin{definition}[Neural Network Architecture]
    A neural network architecture is a tuple $(G,\Sigma)$ where
    \begin{enumerate}
        \item $G=(V,E)$ is an acyclic directed graph.
        \item $V$ is composed of layers $V=\bigsqcup^L_{l=0} V_l$.
        \item For every edge $e\in E$, there exists an $l\in[L-1]$ such that 
        the source node of $e$ is in layer 
        $l$ and the target node is in layer $l+1$.
        \item $\Sigma$ is a collection of non-polynomial
        functions
        $\{\sigma_{i}:\mathbb{R}^{\lvert V_{l}\rvert}\to\mathbb{R}^{\lvert V_{l}\rvert}\}_{l\in [L-1]}$.
    \end{enumerate}
    $L$ is called the depth of the neural network,
    for any $l\in[L]$, we call $V_l$ the $l$-th layer of the neural network.
    We call $V_0$ the input layer and $V_L$ the output layer.
    The layers in between are called hidden layers.
    The size of the $l$-th layer $\lvert V_l \rvert$ is called the width of the layer.

    For any $l\in[L]$, we call $\sigma_l$ the activation function of the $l$-th layer.
\end{definition}
\begin{definition}[Neural Network]
    Given a neural network architecture $(G,\Sigma)$,
    a neural network consists of
    \begin{enumerate}
        \item A set of matrices $W=\{W_l\}_{l=1}^L$ where $W_l$ is a matrix of size
        $\lvert V_l\rvert\times\lvert V_{l+1}\rvert$. Furthermore, $W_{l,i,j}=0$ if
        $(v_{l,i},v_{l+1,j})\not\in E$.
        \item A set of vectors $B=\{B_l\}_{l=1}^L$ where $B_l$ is a vector of size
        $\lvert V_{l+1}\rvert$.
    \end{enumerate}
    We call $W$ the weight matrices and $B$ the bias vectors.
\end{definition}
Given a neural network $(G,\Sigma)$ with a set of weight matrices $W$ and bias vectors $B$,
we define the set of preactivation functions $Z=\{z_l:\mathbb{R}^{\lvert V_l\rvert}\to\mathbb{R}^{\lvert V_{l+1}\rvert}\}_{l\in[L-1]}$ where
\begin{align*}
    z_{l}(\mathbf{x})&= \sigma_{l}(\mathbf{x})\cdot W_{l+1} +B_{l+1}
\end{align*}

And the function represented by the neural network is the concatenation of the preactivation functions:
\[
    f_{W,B}(\mathbf{x})=\bigcirc_{l=0}^{L-1} z_l(\mathbf{x})
\]

The pair of weights and biases $\Theta=(W,B)$ is called the parameters of the neural network.

The main reason for the popularity of neural networks is the universal approximation theorem.
\begin{theorem}[Universal Approximation Theorem]
    Let $\sigma:\mathbb{R}\to\mathbb{R}$ be a non-constant, bounded, and continuous function.
    Let $I\subseteq\mathbb{R}^n$ be a compact set.
    Then for any $\epsilon>0$ and any continuous function $f:I\to\mathbb{R}$,
    there exists an architecture $(G,\Sigma)$ 
    and a set of weights $W$ and biases $B$ such that
    \[
        \lvert f(x)-f_{W,B}(x)\rvert<\epsilon
    \]
    for all $x\in I$.
\end{theorem}

\subsection{Training - Stochastic Gradient Descent}
It is now left to find a method to find the parameters $\Theta$ of a neural network
that minimize the empirical loss function.
The method used in artificial neural networks is called stochastic gradient descent.
Gradient descent is a method for finding the minimum of a function
in an iterative manner.
At each step of the iteration, 
we fix an input, output pair $(x,y)\in\mathbb{D}$ and we compute the 
loss function $\mathcal{L}_{(x,y)}(f_{W,B}(x))$. This stage is called the forward pass
since we are passing the input through the neural network to get the output.
We then compute the gradient of the loss function with respect to the parameters $\Theta$.
This stage is called the backward pass since we are passing the gradient backwards through the neural network
using the chain rule.
We then update the parameters $\Theta$ by subtracting the gradient from the parameters, thus moving
the parameters in the direction of a local minimum.
Ovverall, the algorithm is as follows:
\begin{algorithm}[H]
    \caption{Stochastic Gradient Descent}
    \begin{algorithmic}[1]
        \State Initialize the parameters $\Theta$ randomly.
        \While{not converged}
            \State Sample $(x,y)\in\mathbb{D}$.
            \State Compute the loss function $\mathcal{L}_{(x,y)}(f_{W,B}(x))$.
            \State Compute the gradient $\nabla_{\Theta}\mathcal{L}_{(x,y)}(f_{W,B}(x))$.
            \State Update the parameters $\Theta\gets\Theta-\eta\nabla_{\Theta}\mathcal{L}_{(x,y)}(f_{W,B}(x))$.
        \EndWhile
    \end{algorithmic}
\end{algorithm}
The parameter $\eta$ is called the learning rate and it controls the step size of the gradient descent.
Different learning rates can lead to different convergence rates.
If the learning rate is too small, then the algorithm would take a long time to converge.
If the learning rate is too large, then the algorithm might not converge at all.

Stochastic Gradient Descent (SGD) differs from Gradient Descent in that it updates the model's 
parameters based on a randomly selected subset of training examples $D\subseteq \mathbb{D}$, rather than 
the entire dataset. 
This random selection introduces randomness into the parameter updates, which has been empirically shown
to allow SGD to converge faster 
but with more fluctuations in the optimization process. 
In contrast, Gradient Descent computes the gradient 
using the entire dataset and performs a single parameter update for each iteration, leading to a smoother 
optimization process but potentially slower convergence.
The SGD algorithm is as follows:

\begin{algorithm}[H]
    \caption{Stochastic Gradient Descent}
    \begin{algorithmic}[1]
        \State Initialize the parameters $\Theta$ randomly.
        \While{not converged}
            \State Sample a subset $D\subseteq\mathbb{D}$.
            % iterate over the subset
            \For{$(x,y)\in D$}
                \State Compute the loss function $\mathcal{L}_{(x,y)}(f_{W,B}(x))$.
                \State Compute the gradient $\nabla_{\Theta}\mathcal{L}_{(x,y)}(f_{W,B}(x))$.
                \State Update the parameters $\Theta\gets\Theta-\eta\nabla_{\Theta}\mathcal{L}_{(x,y)}(f_{W,B}(x))$.
            \EndFor
        \EndWhile
    \end{algorithmic}
\end{algorithm}